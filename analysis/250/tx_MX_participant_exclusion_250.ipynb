{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MX Familiar Size Participant Exclusion\n",
    "\n",
    "1. The counterbalancing.csv contains every json that needs to be sampled for familiar size experiment \n",
    "    - The row number for each sequence corresponds to the url fragment used in the variables file uploaded to Heroku server\n",
    "    - This file does not change - only the variables files is updated to resample sequences that get excluded\n",
    "2. Participant exclusion criteron are pre-registered on OSF (https://osf.io/2uqz7) - if the participant is excluded, the counterbalanced sequence needs to be replaced in the variables file \n",
    "3. Keep a log of participant IDs that complete the experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import scipy.stats as stats\n",
    "from scipy import stats\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCSVs(datafolder):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        datafolder = path to data \n",
    "    Returns:\n",
    "        df of all participant data \n",
    "        OR\n",
    "        df for singlle participant\n",
    "    \"\"\"\n",
    "        \n",
    "    #checks if path is a file\n",
    "    isFile = os.path.isfile(datafolder)\n",
    "\n",
    "    #checks if path is a directory\n",
    "    \n",
    "    isDirectory = os.path.isdir(datafolder)\n",
    "    \n",
    "    if isDirectory == True:\n",
    "        data = []\n",
    "        for filename in os.listdir(datafolder):\n",
    "            if 'csv' in filename:\n",
    "                path = datafolder + \"/\" + filename\n",
    "                df = pd.read_csv(path, index_col=None, header=0)\n",
    "                if df.experimentName.unique() == 'final-intermixed-textured':\n",
    "                    subjID = df.subjID.unique()[0]\n",
    "                    data.append(df)\n",
    "\n",
    "        input_frame = pd.concat(data, axis=0, ignore_index=True)\n",
    "        \n",
    "    if isFile == True:\n",
    "        if 'csv' in datafolder:\n",
    "            input_frame = pd.read_csv(datafolder, index_col=None, header=0)\n",
    "    \n",
    "    print('Number of participants before cleaning: ', len(input_frame.subjID.unique()))\n",
    "\n",
    " \n",
    "    return input_frame\n",
    "\n",
    "\n",
    "def feet_to_meters(ft):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        ft = float value in feet \n",
    "        \n",
    "    returns:\n",
    "        m = float value converted to meters \n",
    "    \"\"\"\n",
    "    m = ft * 0.3048\n",
    "    return m\n",
    "\n",
    "def getUnitConveredData(datafolder):\n",
    "    '''\n",
    "    Args: \n",
    "        datafolder = path to data  \n",
    "        \n",
    "    returns:\n",
    "        df with all estimates converted to meters      \n",
    "    '''\n",
    "    input_data = combineCSVs(datafolder) # combine CSVs from all participants \n",
    "    \n",
    "    for idx, row in input_data.iterrows():\n",
    "        unit = row['unitSelection']\n",
    "        # if estimate was made in feet, convert to meters \n",
    "        if unit == 'feet':\n",
    "            estim_ft = row['depth_estimate']\n",
    "            estim_m = feet_to_meters(estim_ft)\n",
    "            # update depth estimates in existing dataframe\n",
    "            input_data.at[idx, 'depth_estimate'] = estim_m\n",
    "\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "def cleanAgeResponses(datafolder):\n",
    "    '''\n",
    "    Args: \n",
    "        datafolder = path to data  \n",
    "        \n",
    "    returns:\n",
    "        df with cleaned reported age \n",
    "    '''\n",
    "    input_data = getUnitConveredData(datafolder)\n",
    "    \n",
    "    for idx, row in input_data.iterrows():\n",
    "        age = row['age']\n",
    "        # if year of birth was given, convert to age\n",
    "        today = datetime.date.today()\n",
    "        year = today.year\n",
    "        if age > 2000:\n",
    "            actual_age = year-age\n",
    "            # update age in existing dataframe\n",
    "            input_data.at[idx, 'age'] = actual_age\n",
    "            print(row['subjID'])\n",
    "            print(actual_age, age)\n",
    "\n",
    "\n",
    "    return input_data    \n",
    " \n",
    "\n",
    "def catchTrial_cleaning(path, correct_requirement, sequence_count):\n",
    "    '''\n",
    "    Participants complete 6 catch trials total to ensure that they are doing the task.\n",
    "    If less than 4/6 catch trials are correct, the participant is excluded.  \n",
    "    '''\n",
    "    \n",
    "    df = cleanAgeResponses(path)\n",
    "    \n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    remove = []\n",
    "    subj_sequence = {}\n",
    "    df2_list = []\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "#         print(subj)\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning\n",
    "        subj_sequence[subj] = subj_df.sequenceName.unique()[0]\n",
    "        \n",
    "        count_correct = 0\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            stim = row['stimulus']\n",
    "            if stim.split('/')[0] == 'catch_stimuli':\n",
    "                ####### VERSION WHERE CATCH TRIALS ARE ATTENTION CHECK: IMAGE HAS NO TARGET\n",
    "                if row[\"depth_estimate\"] == 0:\n",
    "                    count_correct += 1\n",
    "\n",
    "                # remove catch trial \n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "\n",
    "        if count_correct < correct_requirement:\n",
    "            remove.append(subj)\n",
    "            print(count_correct)\n",
    "        else:\n",
    "            sequence_count[subj_df.sequenceName.unique()[0]] += 1\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "    print(\"Number of participants that did not pass the catch trial check:\", len(remove))\n",
    "    print(\"Participants that were removed:\",remove)\n",
    "            \n",
    "    for subj in remove:\n",
    "        df2.drop(df2[df2['subjID'] == subj].index, inplace = True) \n",
    "    \n",
    "    return df2\n",
    "    \n",
    "\n",
    "def removeMissedTrials(input_data, num_trials):\n",
    "    \"\"\"\n",
    "    Participants were told that if they missed a trial, to respond '0'.\n",
    "    This function removes those trials, and keeps track of:\n",
    "    (1) How many missed trials per participant\n",
    "    (2) Number of missed trials per duration \n",
    "    (3) Number of missed trials per sequence \n",
    "    \"\"\"\n",
    "#     input_data = cleanAgeResponses(datafolder)\n",
    "    \n",
    "    missedTrials_participants = {}\n",
    "    missedTrials_durations = {}\n",
    "    missedTrials_sequences = {}\n",
    "    \n",
    "    \n",
    "    for idx, row in input_data.iterrows():\n",
    "        estimate = row['depth_estimate']\n",
    "        if estimate == 0.0:\n",
    "            subjID = row['subjID']\n",
    "            duration = row['duration']\n",
    "            sequenceName = row['sequenceName']\n",
    "            \n",
    "            if subjID not in missedTrials_participants:\n",
    "                missedTrials_participants[subjID] = 1\n",
    "            else:\n",
    "                missedTrials_participants[subjID] += 1\n",
    "\n",
    "            if duration not in missedTrials_durations:\n",
    "                missedTrials_durations[duration] = 1\n",
    "            else:\n",
    "                missedTrials_durations[duration] += 1\n",
    "            \n",
    "            if sequenceName not in missedTrials_sequences:\n",
    "                missedTrials_sequences[sequenceName] = 1\n",
    "            else:\n",
    "                missedTrials_sequences[sequenceName] += 1\n",
    "            \n",
    "#             print(subjID, duration, sequenceName)\n",
    "            \n",
    "            # remove trials with depth estimate = 0 \n",
    "            input_data.drop(idx, inplace=True)\n",
    "    \n",
    "    # remove participants data if the participant's missed trial count is 10% or more of num_trials\n",
    "    threshold = math.floor(num_trials * 0.1)\n",
    "#     print(\"Missing Trial Count Threshold: \", threshold)\n",
    "    remove_ids = []\n",
    "    for key in missedTrials_participants:\n",
    "        if missedTrials_participants[key] >= threshold:\n",
    "            remove_ids.append(key)\n",
    "    print(\"Number of participants with 10% or more missed trials: \", len(remove_ids))\n",
    "            \n",
    "    for subj in remove_ids:\n",
    "        input_data.drop(input_data[input_data['subjID'] == subj].index, inplace = True) \n",
    "\n",
    "    # Note if a particular participant, duration, or sequence has maximum missing trials\n",
    "    # ** If the participant had no missed trials, then ID will not show up in dict \n",
    "#     print(\"Missed Trials\")\n",
    "#     print(missedTrials_participants)\n",
    "#     print(missedTrials_durations)\n",
    "#     print(missedTrials_sequences)\n",
    "\n",
    "    \n",
    "    return input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/MX_fS_VE_MTurk/data_250'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_path = '/Users/prachimahableshwarkar/Documents/GW/Depth_MTurk/familiarSize/counterbalancing/MX_250_jsons'\n",
    "\n",
    "sequences_count_dict = {}\n",
    "for seq in os.listdir(sequences_path):\n",
    "    if 'VE250' in seq:\n",
    "        sequences_count_dict['jsons/'+seq] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants before cleaning:  564\n",
      "3\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "Number of participants that did not pass the catch trial check: 40\n",
      "Participants that were removed: [454373, 467553, 502533, 195944, 950491, 412810, 523679, 803507, 166406, 989871, 710174, 809953, 112043, 622629, 785840, 501359, 556510, 560126, 155895, 631509, 154424, 571059, 884098, 558555, 185041, 473353, 149057, 169502, 301219, 625263, 963980, 839769, 550860, 165356, 445073, 566780, 877333, 506788, 766346, 499509]\n"
     ]
    }
   ],
   "source": [
    "catch_trial_cleaned_data = catchTrial_cleaning(path, 4, sequences_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the image name as a column in the df \n",
    "catch_trial_cleaned_data['imageName'] = catch_trial_cleaned_data.apply(lambda row: row.stimulus.split('/')[1].split('_')[0], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants with 10% or more missed trials:  72\n"
     ]
    }
   ],
   "source": [
    "missed_trial_cleaned_data = removeMissedTrials(catch_trial_cleaned_data, num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RT_Cleaning(df, outlier_range, num_trials):\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    df2_list = []\n",
    "    for subj in all_subjIDs:\n",
    "        count = 0\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        cleaned_subj_df = subj_df.copy(deep=True) # prevent setting with copy warning \n",
    "        # calculate subject's average trial RT \n",
    "        average_trial_RT = subj_df[\"trial_RT\"].mean()\n",
    "        std_trial_RT = subj_df[\"trial_RT\"].std()\n",
    "\n",
    "        for idx, row in subj_df.iterrows():\n",
    "            RT = row[\"trial_RT\"]\n",
    "            if RT < outlier_range[0]: # outlier\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "#                 print(RT)\n",
    "            if RT > outlier_range[1]:\n",
    "                cleaned_subj_df.drop([idx], inplace=True)\n",
    "                count += 1\n",
    "#                 print(RT)\n",
    "                \n",
    "        threshold = math.floor(num_trials * 0.1)\n",
    "        if count >= threshold:\n",
    "            remove.append(subj)\n",
    "        \n",
    "        df2_list.append(cleaned_subj_df)\n",
    "    \n",
    "    df2 = pd.concat(df2_list)\n",
    "            \n",
    "    print(\"Number of Participants with 10% or more trials outside their RT range: \", len(remove))\n",
    "    \n",
    "#     for index, row in df2.iterrows():\n",
    "#         if row['subjID'] in remove:\n",
    "#             df2.drop(index, inplace=True)\n",
    "            \n",
    "    for subj in remove:\n",
    "        df2.drop(df2[df2['subjID'] == subj].index, inplace = True) \n",
    "        print(subj)\n",
    "                \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with 10% or more trials outside their RT range:  35\n",
      "627033\n",
      "652752\n",
      "741571\n",
      "120918\n",
      "194749\n",
      "173316\n",
      "522025\n",
      "422357\n",
      "143744\n",
      "565508\n",
      "130599\n",
      "498085\n",
      "384207\n",
      "839508\n",
      "649817\n",
      "306463\n",
      "239363\n",
      "687113\n",
      "133472\n",
      "473691\n",
      "392686\n",
      "214742\n",
      "743005\n",
      "958196\n",
      "912799\n",
      "100683\n",
      "742153\n",
      "933692\n",
      "280470\n",
      "146128\n",
      "348303\n",
      "247282\n",
      "810579\n",
      "364102\n",
      "140455\n"
     ]
    }
   ],
   "source": [
    "RT_cleaned_data = RT_Cleaning(missed_trial_cleaned_data, [250, 10000], num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatResponses_Cleaning(df, min_unique_responses):\n",
    "    \"\"\"\n",
    "    Some participants gave'junk data' - same number repeated for many trials \n",
    "    Count the frequency of unique responses entered by the participant. \n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    max_repeats_distribution = []\n",
    "    num_unique_responses_distribution = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        # ideally, the max repeats and num_unique_responses should be ~ 48 since there are 48 imgs at each depth bin \n",
    "        count_depth_estimates = subj_df['depth_estimate'].value_counts()\n",
    "        num_unique_responses = len(count_depth_estimates)\n",
    "        num_unique_responses_distribution.append(num_unique_responses)\n",
    "        max_repeats = count_depth_estimates.max()\n",
    "        max_repeats_distribution.append(max_repeats)\n",
    "        if num_unique_responses < min_unique_responses:\n",
    "#             print(num_unique_responses)\n",
    "            remove.append(subj)\n",
    "    print('Number of participants with less than 6 unique responses:', len(remove))\n",
    "    \n",
    "    avg_max_repeats = np.array(max_repeats_distribution).mean()\n",
    "    std_max_repeats = np.array(max_repeats_distribution).std()\n",
    "    \n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_depth_estimates = subj_df['depth_estimate'].value_counts()\n",
    "        max_repeats = count_depth_estimates.max()\n",
    "\n",
    "        outlierrange = [avg_max_repeats - (3*std_max_repeats), avg_max_repeats + (3*std_max_repeats)]\n",
    "        if max_repeats < outlierrange[0]:\n",
    "            if subj not in remove:\n",
    "                remove.append(subj)\n",
    "                print(True)\n",
    "        if max_repeats > outlierrange[1]:\n",
    "            if subj not in remove:\n",
    "                remove.append(subj)\n",
    "\n",
    "    print(\"Number of total participants removed: repeat responses: \", len(remove))\n",
    "    \n",
    "#     for index, row in df.iterrows():\n",
    "#         if row['subjID'] in remove:\n",
    "#             df.drop(index, inplace=True)\n",
    "            \n",
    "    for subj in remove:\n",
    "        df.drop(df[df['subjID'] == subj].index, inplace = True) \n",
    "\n",
    "    \n",
    "    return df, max_repeats_distribution, num_unique_responses_distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants with less than 6 unique responses: 67\n",
      "Number of total participants removed: repeat responses:  68\n"
     ]
    }
   ],
   "source": [
    "repeat_resp_cleaned_data, max_repeats_distrib, num_unique_distrib = repeatResponses_Cleaning(RT_cleaned_data, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalTrialCountCheck(df, num_trials):\n",
    "    \"\"\"\n",
    "    If more then 10% of a participants data is missing, remove the participant\n",
    "    \"\"\"\n",
    "    #List unique values in the df['subjID'] column\n",
    "    all_subjIDs = df.subjID.unique()\n",
    "    \n",
    "    remove = []\n",
    "    for subj in all_subjIDs:\n",
    "        subj_df = df.loc[df['subjID'] == subj]\n",
    "        count_trials = len(subj_df.index)\n",
    "        threshold_trials_remaining = num_trials - math.floor(num_trials * 0.1)\n",
    "\n",
    "        if count_trials <= threshold_trials_remaining:\n",
    "            remove.append(subj)\n",
    "            \n",
    "    print(\"Number of Participants with >= 10% trials removed: \", len(remove))\n",
    "\n",
    "            \n",
    "    for subj in remove:\n",
    "        df.drop(df[df['subjID'] == subj].index, inplace = True) \n",
    "                    \n",
    "    print(\"Number of participants left: \",len(df.subjID.unique()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Participants with >= 10% trials removed:  25\n",
      "Number of participants left:  324\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = finalTrialCountCheck(repeat_resp_cleaned_data, num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subj in cleaned_data.subjID.unique():\n",
    "#     subj_df = cleaned_data.loc[cleaned_data['subjID']==subj]\n",
    "#     print(subj_df.sequenceName.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = cleaned_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequence sampling file: 250 ms ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons_dir = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/MX_fS_VE_MTurk/jsons'\n",
    "\n",
    "file_dest = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/MX_fS_VE_MTurk/sequence_tracking/'\n",
    "\n",
    "sequence_sampling_dict = {}\n",
    "for seq_name in os.listdir(jsons_dir):\n",
    "    if '.json' in seq_name:\n",
    "        sequence_sampling_dict[seq_name] = []\n",
    "\n",
    "# Convert and write JSON object to file\n",
    "v0_filename = 'v0_MX_master_sequence_tracking.json'\n",
    "with open(file_dest + v0_filename, \"w\") as outfile: \n",
    "    json.dump(sequence_sampling_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n"
     ]
    }
   ],
   "source": [
    "print(len(sequence_sampling_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Sequence Sampling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/MX_fS_VE_MTurk/sequence_tracking/v8_MX_master_sequence_tracking.json\n"
     ]
    }
   ],
   "source": [
    "# set the version for the sequence tracking \n",
    "\n",
    "prev_version = 'v8'\n",
    "new_version = 'v9'\n",
    "\n",
    "# select path for the last previous sequence tracking file \n",
    "\n",
    "sequence_sampling_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/MX_fS_VE_MTurk/sequence_tracking/'+ prev_version + '_MX_master_sequence_tracking.json'\n",
    "print(sequence_sampling_path)\n",
    "# Opening JSON file\n",
    "f = open(sequence_sampling_path)\n",
    "  \n",
    "# returns JSON object as a dictionary\n",
    "sequence_sampling = json.load(f)\n",
    "\n",
    "# sequence_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences previously sampled:  323\n",
      "Number of sequences sampled now:  324 / 324\n",
      "Number of sequences to be sampled:  0\n",
      "Number sampled + to be sampled = 324:  True\n"
     ]
    }
   ],
   "source": [
    "# print number of sequences that have been sampled by the previous batch\n",
    "prev_sampled_count = 0\n",
    "for seq in sequence_sampling:\n",
    "    if len(sequence_sampling[seq]) > 0:\n",
    "        prev_sampled_count += 1\n",
    "print('Number of sequences previously sampled: ', prev_sampled_count)       \n",
    "\n",
    "new_sequence_sampling = sequence_sampling\n",
    "# update sequence sampling dictionary\n",
    "for subj in final_data.subjID.unique():\n",
    "    subj_df = final_data.loc[final_data['subjID'] == subj]\n",
    "    subj_seq = subj_df.sequenceName.unique()[0].split('/')[1]\n",
    "    # add subj to list for its corresponding sequence\n",
    "    new_sequence_sampling[subj_seq].append(str(subj))\n",
    "\n",
    "    \n",
    "sampled_count = 0\n",
    "unsampled_count = 0\n",
    "for seq in new_sequence_sampling:\n",
    "    if len(new_sequence_sampling[seq]) > 0:\n",
    "        # remove duplicates of the same id\n",
    "        new_sequence_sampling[seq] = list(set(new_sequence_sampling[seq]))\n",
    "        sampled_count += 1\n",
    "    else:\n",
    "        unsampled_count += 1\n",
    "        new_sequence_sampling[seq] = []\n",
    "        \n",
    "print('Number of sequences sampled now: ', sampled_count, '/', len(final_data.sequenceName.unique()))\n",
    "\n",
    "print('Number of sequences to be sampled: ', unsampled_count)\n",
    "\n",
    "print('Number sampled + to be sampled = 324: ', sampled_count + unsampled_count==324)\n",
    "\n",
    "\n",
    "seq_track_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/MX_fS_VE_MTurk/sequence_tracking/'\n",
    "\n",
    "with open(seq_track_path + new_version + \"_MX_master_sequence_tracking.json\", \"w\") as outfile:\n",
    "    json.dump(new_sequence_sampling, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find sequences to replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, True)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_to_replace = []\n",
    "\n",
    "for seq_key in new_sequence_sampling:\n",
    "    if len(new_sequence_sampling[seq_key]) == 0:\n",
    "        sequences_to_replace.append(seq_key)\n",
    "\n",
    "len(sequences_to_replace), unsampled_count == len(sequences_to_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new batch variables file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Sampled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jsons/MX_seq10_1_VE250_flipped.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jsons/MX_seq7_0_VE250_flipped.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jsons/MX_seq21_0_VE250_flipped.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jsons/MX_seq51_1_VE250_flipped.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jsons/MX_seq43_2_VE250_flipped.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>jsons/MX_seq20_2_VE250.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>jsons/MX_seq22_0_VE250_flipped.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>jsons/MX_seq40_2_VE250_flipped.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>jsons/MX_seq52_1_VE250_flipped.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>jsons/MX_seq39_2_VE250_flipped.json</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Path  Sampled\n",
       "0    jsons/MX_seq10_1_VE250_flipped.json        0\n",
       "1     jsons/MX_seq7_0_VE250_flipped.json        0\n",
       "2    jsons/MX_seq21_0_VE250_flipped.json        0\n",
       "3    jsons/MX_seq51_1_VE250_flipped.json        0\n",
       "4    jsons/MX_seq43_2_VE250_flipped.json        0\n",
       "..                                   ...      ...\n",
       "319          jsons/MX_seq20_2_VE250.json        0\n",
       "320  jsons/MX_seq22_0_VE250_flipped.json        0\n",
       "321  jsons/MX_seq40_2_VE250_flipped.json        0\n",
       "322  jsons/MX_seq52_1_VE250_flipped.json        0\n",
       "323  jsons/MX_seq39_2_VE250_flipped.json        0\n",
       "\n",
       "[324 rows x 2 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterbalancing_path = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/MX_fS_VE_MTurk/counterbalancing.csv'\n",
    "counterbalancing_df = pd.read_csv(counterbalancing_path)\n",
    "counterbalancing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Notes\n",
    "\n",
    "The row in the counterbalancing csv does NOT match the url fragment since the indexing includes the path row.\n",
    "\n",
    "The url fragment is the counterbalancing df index + 1 --> this has been validated in the console log of the experiment\n",
    "\n",
    "To backtrack from the url fragments to the corresponding row of the counterbalancing csv: row = url_fragment + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_fragments = []\n",
    "for sequence in sequences_to_replace:\n",
    "    seq_p = 'jsons/' + sequence\n",
    "    url_fragments.append(counterbalancing_df.index[counterbalancing_df['Path']==seq_p][0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number for the NEXT batch \n",
    "# 1-4 were to get the sequences sampled for the first time \n",
    "# 5 was split into 5, 6 to keep batch size small \n",
    "batch = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_variables_csv = '/Users/prachimahableshwarkar/Documents/GW/FacialAge/FacialAge_MTurk/BNav_EC2/DepthDuration/MX_fS_VE_MTurk/batch_variables/'\n",
    "\n",
    "base_url = 'http://54.235.29.9/FacialAge/BNav_EC2/DepthDuration/MX_fS_VE_MTurk/MX_fS_HTML.html#'\n",
    "\n",
    "variables = {'experiment_url': [], 'sampled': []}\n",
    "\n",
    "for fragment in url_fragments:\n",
    "     variables['experiment_url'].append(base_url + str(fragment))\n",
    "     variables['sampled'].append('unsampled')\n",
    "\n",
    "\n",
    "\n",
    "variables_df = pd.DataFrame(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_df.to_csv(dest_variables_csv + 'MX_textured_variables' + '_b' + str(batch) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_dest = '/Users/prachimahableshwarkar/Documents/GW/spatial_perception/app/'\n",
    "\n",
    "variables_df.to_csv(server_dest + 'variables.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
